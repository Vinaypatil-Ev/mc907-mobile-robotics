{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 - Mc907/Mo651 - Mobile Robotics\n",
    "\n",
    "### Student:\n",
    "Luiz Eduardo Cartolano - RA: 183012\n",
    "\n",
    "### Instructor:\n",
    "Esther Luna Colombini\n",
    "\n",
    "### Github Link:\n",
    "[Project Repository](https://github.com/luizcartolano2/mc907-mobile-robotics)\n",
    "\n",
    "### Youtube Link:\n",
    "[Link to Video](https://youtu.be/uqNeEhWo0dA)\n",
    "\n",
    "### Subject of this Work:\n",
    "The general objective of this work is to implement a deep learning approach for solve the Visual Odometry problem.\n",
    "\n",
    "### Goals:\n",
    "1. Implement and evaluate a Deep VO strategy using images from the [AirSim](https://github.com/microsoft/AirSim) simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean wrong images\n",
    "\n",
    "While upload images obtained from the AirSim simulator were noted that some of them had failure, so, we have to clean this data to avoid noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dt in ['1','2','3','4','5','6']:\n",
    "    path = 'dataset/'+'seq'+dt+'/'\n",
    "    print(\"-------------------------------------------\")\n",
    "    print('|    '+path)\n",
    "    all_images = glob.glob(path+'images'+'/*')\n",
    "    df_poses = pd.read_csv(path+'poses.csv')[['ImageFile']].values\n",
    "    for img in df_poses:\n",
    "        if not (path+'images/'+img) in all_images:\n",
    "            print('|        '+img[0])\n",
    "    print(\"-------------------------------------------\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rgb_mean(image_sequence):\n",
    "    '''\n",
    "        Compute the mean over each channel separately over a set of images.\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_sequence  :   np.ndarray\n",
    "                        Array of shape ``(N, h, w, c)`` or ``(h, w, c)``\n",
    "    '''\n",
    "    if image_sequence.ndim == 4:\n",
    "        _, h, w, c = image_sequence.shape\n",
    "    if image_sequence.ndim == 3:\n",
    "        h, w, c = image_sequence.shape\n",
    "    # compute mean separately for each channel\n",
    "    # somehow this expression is buggy, so we must do it manually\n",
    "    # mode = image_sequence.mean((0, 1, 2))\n",
    "    mean_r = image_sequence[..., 0].mean()\n",
    "    mean_g = image_sequence[..., 1].mean()\n",
    "    mean_b = image_sequence[..., 2].mean()\n",
    "    mean = np.array([mean_r, mean_g, mean_b])\n",
    "    return mean\n",
    "\n",
    "def mean_normalize(images_vector):\n",
    "    '''\n",
    "        Normalize data to the range -1 to 1\n",
    "    '''\n",
    "    out_images = []\n",
    "    \n",
    "    N = len(images_vector)\n",
    "    \n",
    "    print('|    Mean-normalizing ...')\n",
    "\n",
    "    mean_accumlator = np.zeros((3,), dtype=np.float32)\n",
    "\n",
    "    for idx in range(N):\n",
    "        img = images_vector[idx]\n",
    "        mean_accumlator += compute_rgb_mean(img)\n",
    "\n",
    "    mean_accumlator /= N\n",
    "    print(f'|    Mean: {mean_accumlator}')\n",
    "    \n",
    "    for idx in range(N):\n",
    "        img = images_vector[idx]\n",
    "        out_images.append(img - mean_accumlator)\n",
    "    \n",
    "    print('|    Done')\n",
    "    \n",
    "    return out_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path,img_size=(256,144)):\n",
    "    \"\"\"\n",
    "        Function to read an image from a given path.\n",
    "        \n",
    "        :param: path - image path\n",
    "        :param: img_size - image size\n",
    "        \n",
    "        :return: img - numpy array with the images pixels (converted to grayscale and normalized)\n",
    "        \n",
    "    \"\"\"\n",
    "    # read image from path\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    # normalize image pixels\n",
    "    img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(img_dir, img_size=(256,144)):\n",
    "\n",
    "    \"\"\"\n",
    "        Function to coordinate the load of all the images that are going to be used.\n",
    "        \n",
    "        :param: img_dir - path to the directory containing the images\n",
    "        :param: img_size - image size\n",
    "        \n",
    "        :return: images_set - numpy array with all images at the set\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print (\"|    Loading images from: \", img_dir)\n",
    "    \n",
    "    # loop to read all the images of the directory\n",
    "    images = [get_image(img,img_size) for img in glob.glob(img_dir+'/*')]\n",
    "    \n",
    "    # normalize images\n",
    "    images = mean_normalize(images)\n",
    "    \n",
    "    #resemble images as RGB\n",
    "    images = [img[:, :, (2, 1, 0)] for img in images]\n",
    "    \n",
    "    #Transpose the image that channels num. as first dimension\n",
    "    images = [np.transpose(img,(2,0,1)) for img in images]\n",
    "    images = [torch.from_numpy(img) for img in images]\n",
    "    \n",
    "    #stack per 2 images\n",
    "    images = [np.concatenate((images[k],images[k+1]),axis = 0) for k in range(len(images)-1)]\n",
    "        \n",
    "    print(\"|    Images count : \",len(images))\n",
    "\n",
    "    # reshape the array of all images\n",
    "    images_set = np.stack(images,axis=0)\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "    return images_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq1/images/\n"
     ]
    }
   ],
   "source": [
    "load_images('dataset/seq1/images/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions are used to the Kitti Dataset poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRotationMatrix(R):\n",
    "    \"\"\" \n",
    "        Checks if a matrix is a valid rotation matrix referred from \n",
    "        https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "        \n",
    "        :param: R - rotation matrix\n",
    "        \n",
    "        :return: True or False\n",
    "        \n",
    "    \"\"\"\n",
    "    # calc the transpose\n",
    "    Rt = np.transpose(R)\n",
    "\n",
    "    # check identity\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    \n",
    "    return n < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\" \n",
    "        Calculates rotation matrix to euler angles\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles\n",
    "        \n",
    "        :param: R - rotation matrix\n",
    "        \n",
    "        :return: rotation matrix for Euler angles\n",
    "    \"\"\"\n",
    "    assert(isRotationMatrix(R))\n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrices(all_poses):\n",
    "    \"\"\"\n",
    "        Function to extract matrices from poses\n",
    "        \n",
    "        :param: all_poses - list with all poses from the sequence\n",
    "        \n",
    "        :return: all_matrices - list with all matrices obtained from the poses\n",
    "    \"\"\"\n",
    "    all_matrices = []\n",
    "    for i in range(len(all_poses)):\n",
    "        #print(\"I: \",i)\n",
    "        j = all_poses[i]\n",
    "        #print(\"J:   \",j)\n",
    "        p = np.array([j[3], j[7], j[11]])\n",
    "        #print(\"P:   \", p)\n",
    "        R = np.array([[j[0],j[1],j[2]],\n",
    "                      [j[4],j[5],j[6]],\n",
    "                      [j[8],j[9],j[10]]\n",
    "                     ])\n",
    "        #print(\"R:   \", R)\n",
    "        angles = rotationMatrixToEulerAngles(R)\n",
    "        #print(\"Angles: \",angles)\n",
    "        matrix = np.concatenate((p,angles))\n",
    "        #print(\"MATRIX: \", matrix)\n",
    "        all_matrices.append(matrix)\n",
    "    return all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kitti_images(pose_file):\n",
    "    poses = []\n",
    "    poses_set = []\n",
    "\n",
    "    with open(pose_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose = np.fromstring(line, dtype=float, sep=' ')\n",
    "            poses.append(pose)\n",
    "    \n",
    "    poses = getMatrices(poses)\n",
    "    pose1 = poses[0]\n",
    "    for i in range(len(poses)-1):\n",
    "        pose2 = poses[i+1]\n",
    "        finalpose = np.zeros(pose1)\n",
    "        poses_set.append(finalpose)\n",
    "\n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions are used for the poses obtained from the AirSim simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pi_to_poses(pose):\n",
    "    '''Add Pi to every pose angle.'''\n",
    "    pose += np.pi\n",
    "    \n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_to_euler_angles(quat_matrix):\n",
    "    # create a scipy object from the quaternion angles\n",
    "    rot_mat = R.from_quat(quat_matrix)\n",
    "    # convert the quaternion to euler (in degrees)\n",
    "    euler_mat = rot_mat.as_euler('yxz', degrees=False)\n",
    "    # convert from (-pi,pi) to (0,2pi)\n",
    "    euler_convert = add_pi_to_poses(euler_mat)\n",
    "    \n",
    "    return euler_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_airsim_pose(pose_file):\n",
    "    \n",
    "    poses = []\n",
    "    poses_set = []\n",
    "    \n",
    "    df_poses = pd.read_csv(pose_file)\n",
    "    for index, row in df_poses.iterrows():\n",
    "        # get the (x,y,z) positions of the camera\n",
    "        position = np.array([row['POS_X'],row['POS_Y'],row['POS_Z']])\n",
    "        # get the quaternions angles of the camera\n",
    "        quat_matrix = np.array([row['Q_X'],row['Q_Y'], row['Q_Z'],row['Q_W']])\n",
    "        # call the func that convert the quaternions to euler angles\n",
    "        euler_matrix = quat_to_euler_angles(quat_matrix)\n",
    "        # concatenate both position(x,y,z) and euler angles\n",
    "        poses.append(np.concatenate((position,euler_matrix)))\n",
    "    \n",
    "    # make the first pose as start position\n",
    "    pose1 = poses[0]\n",
    "    for i in range(len(poses)):\n",
    "        pose2 = poses[i]\n",
    "        pose_diff = np.subtract(pose2, pose1)\n",
    "        pose_diff[4:] = np.arctan2(np.sin(pose_diff[4:]), np.cos(pose_diff[4:]))\n",
    "        \n",
    "        poses[i] = pose_diff\n",
    "    \n",
    "    # get the desloc between two poses\n",
    "    for i in range(len(poses)-1):\n",
    "        pose1 = poses[i]\n",
    "        pose2 = poses[i+1]\n",
    "\n",
    "        pose_diff = np.subtract(pose2, pose1)\n",
    "        pose_diff[4:] = np.arctan2(np.sin(pose_diff[4:]), np.cos(pose_diff[4:]))\n",
    "\n",
    "        poses_set.append(pose_diff)\n",
    "        \n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poses(pose_file, pose_format='airsim'):\n",
    "    \"\"\"\n",
    "        Function to load the image poses.\n",
    "        \n",
    "        :param: pose_file - path to the pose file\n",
    "        :param: pose_format - where the pose were obtained from (AirSim, VREP, Kitti, etc...)\n",
    "        \n",
    "        :return: pose_set - set of the poses for the sequence\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print (\"|    Pose from: \",pose_file)\n",
    "    \n",
    "    if pose_format.lower() == 'kitti':\n",
    "        poses_set = load_kitti_images(pose_file)\n",
    "    elif pose_format.lower() == 'airsim':\n",
    "        poses_set = load_airsim_pose(pose_file)\n",
    "        \n",
    "    print(\"|        Poses count: \",len(poses_set))\n",
    "    print(\"----------------------------------------------------------------------\")    \n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "Function that acquire all data that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VODataLoader(datapath,img_size=(256,144), test=False):\n",
    "    if test:\n",
    "        sequences = ['3']\n",
    "    else:\n",
    "        sequences = ['1','2','4','5','6']\n",
    "        \n",
    "    images_set = []\n",
    "    odometry_set = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        dir_path = os.path.join(datapath,'seq'+sequence)\n",
    "        image_path = os.path.join(dir_path,'images')\n",
    "        pose_path = os.path.join(dir_path,'poses.csv')\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        print(\"|Load from: \", dir_path)\n",
    "        images_set.append(torch.FloatTensor(load_images(image_path,img_size)))\n",
    "        odometry_set.append(torch.FloatTensor(load_poses(pose_path, 'AirSim')))\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"|   Total Images: \", len(images_set))\n",
    "    print(\"|   Total Odometry: \", len(odometry_set))\n",
    "    print(\"---------------------------------------------------\")    \n",
    "    return images_set, odometry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq1\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq1/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.3115534  0.29183614 0.27286035]\n",
      "|    Done\n",
      "|    Images count :  326\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq1/poses.csv\n",
      "|        Poses count:  326\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq2\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq2/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.2615339  0.24993458 0.2350733 ]\n",
      "|    Done\n",
      "|    Images count :  283\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq2/poses.csv\n",
      "|        Poses count:  283\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq4\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq4/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.34079915 0.32100374 0.28959265]\n",
      "|    Done\n",
      "|    Images count :  368\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq4/poses.csv\n",
      "|        Poses count:  368\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq5\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq5/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.13811225 0.20660812 0.3470334 ]\n",
      "|    Done\n",
      "|    Images count :  121\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq5/poses.csv\n",
      "|        Poses count:  121\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq6\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq6/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.59857106 0.6014284  0.56355304]\n",
      "|    Done\n",
      "|    Images count :  208\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq6/poses.csv\n",
      "|        Poses count:  208\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "|   Total Images:  5\n",
      "|   Total Odometry:  5\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X,y = VODataLoader(datapath='dataset', test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting lists containing tensors to tensors as per the batchsize (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [item for x in X for item in x]\n",
    "Y_train = [item for a in y for item in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info about the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Details of X :\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "1306\n",
      "torch.Size([144, 256, 6])\n",
      "---------------------------------\n",
      "Details of y :\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "1306\n",
      "torch.Size([6])\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"Details of X :\")\n",
    "print(type(X_train)) \n",
    "print(type(X_train[0]))\n",
    "print(len(X_train)) \n",
    "print(X_train[0].size())\n",
    "print(\"---------------------------------\")\n",
    "print(\"Details of y :\")\n",
    "print(type(Y_train))\n",
    "print(type(Y_train[0]))\n",
    "print(len(Y_train))\n",
    "print(Y_train[0].size())\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = torch.stack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stack = torch.stack(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = X_stack.view(-1,1,6,144,256)\n",
    "y_batch = y_stack.view(-1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([1306, 1, 6, 144, 256])\n",
      "Details of y :\n",
      "torch.Size([1306, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Details of X :\")\n",
    "print(X_batch.size())\n",
    "print(\"Details of y :\")\n",
    "print(y_batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .2\n",
    "dataset_size = len(X_batch)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch_train = X_batch[split:]\n",
    "y_batch_train = y_batch[split:]\n",
    "X_batch_validation = X_batch[:split]\n",
    "y_batch_validation = y_batch[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining DeepVO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepVONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepVONet, self).__init__()\n",
    "        # CNN\n",
    "        # convolutional layer 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False), #6 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2), \n",
    "        )\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 3_1        \n",
    "        self.conv3_1 = nn.Sequential(\n",
    "            nn.Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 4_1\n",
    "        self.conv4_1 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 5\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 5_1\n",
    "        self.conv5_1 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 6\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d (512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        # RNN\n",
    "        self.lstm1 = nn.LSTMCell(2*6*1024, 100)\n",
    "        self.lstm2 = nn.LSTMCell(100, 100)\n",
    "        self.fc = nn.Linear(in_features=100, out_features=6)\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def reset_hidden_states(self, size=1, zero=True):\n",
    "        if zero == True:\n",
    "            self.hx1 = Variable(torch.zeros(size, 100))\n",
    "            self.cx1 = Variable(torch.zeros(size, 100))\n",
    "            self.hx2 = Variable(torch.zeros(size, 100))\n",
    "            self.cx2 = Variable(torch.zeros(size, 100))\n",
    "        else:\n",
    "            self.hx1 = Variable(self.hx1.data)\n",
    "            self.cx1 = Variable(self.cx1.data)\n",
    "            self.hx2 = Variable(self.hx2.data)\n",
    "            self.cx2 = Variable(self.cx2.data)\n",
    "\n",
    "        if next(self.parameters()).is_cuda == True:\n",
    "            self.hx1 = self.hx1.cuda()\n",
    "            self.cx1 = self.cx1.cuda()\n",
    "            self.hx2 = self.hx2.cuda()\n",
    "            self.cx2 = self.cx2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = x.view(x.size(0), 2 * 6 * 1024)\n",
    "        \n",
    "        self.hx1, self.cx1 = self.lstm1(x, (self.hx1, self.cx1))\n",
    "        x = self.hx1\n",
    "        self.hx2, self.cx2 = self.lstm2(x, (self.hx2, self.cx2))\n",
    "        x = self.hx2\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVONet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv3_1): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv4_1): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv5_1): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (lstm1): LSTMCell(12288, 100)\n",
      "  (lstm2): LSTMCell(100, 100)\n",
      "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = DeepVONet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss and optimizer to be used \n",
    "criterion = torch.nn.MSELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, weight_decay=0.5)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the pretrained weight of FlowNet ( CNN part )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained = torch.load('flownets_EPE1.951.pth.tar',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dict['conv1.weight'] = pre_trained['state_dict']['conv1.0.weight']\n",
    "update_dict['conv2.weight'] = pre_trained['state_dict']['conv2.0.weight']\n",
    "update_dict['conv3.weight'] = pre_trained['state_dict']['conv3.0.weight']\n",
    "update_dict['conv4.weight'] = pre_trained['state_dict']['conv4.0.weight']\n",
    "update_dict['conv4_1.weight'] = pre_trained['state_dict']['conv4_1.0.weight']\n",
    "update_dict['conv5.weight'] = pre_trained['state_dict']['conv5.0.weight']\n",
    "update_dict['conv5_1.weight'] = pre_trained['state_dict']['conv5_1.0.weight']\n",
    "update_dict['conv6.weight'] = pre_trained['state_dict']['conv6.0.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(update_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, train_num, X, y, epoch_num=25):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        for i in range(train_num):\n",
    "            print(\"    Train num :\", i+1)\n",
    "            inputs = X[i]\n",
    "            print(\"        Input Size: {}\".format(inputs.size()))\n",
    "            labels = y[i]\n",
    "            print(\"        Labels: \",labels)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            print(\"        Outputs: \",outputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('    Epoch : %d Loss: %.3f' %(epoch+1, running_loss/train_num))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    print (\"Time taken in Training {0}\".format((time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_v2(model, X_train, y_train, X_validate, y_validate, num_epochs):\n",
    "    # start model train mode\n",
    "    train_ep_loss = []\n",
    "    valid_ep_loss = []\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "        st_t = time.time()\n",
    "        print('='*50)\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        \n",
    "        loss_mean = 0\n",
    "        t_loss_list = []\n",
    "        \n",
    "        for i in range(X_train.size(0)):\n",
    "            # get the images inputs\n",
    "            inputs = X_train[i]\n",
    "            # get the original poses\n",
    "            labels = y_train[i]\n",
    "            # zero optimizer\n",
    "            optimizer.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "            \n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            # get mse loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            ls = loss.item()\n",
    "            loss.backward()\n",
    "            # set next optimizer step\n",
    "            optimizer.step()\n",
    "            # append loss\n",
    "            t_loss_list.append(float(ls))\n",
    "            # update loss\n",
    "            loss_mean += float(ls)\n",
    "        \n",
    "        print('Train take {:.1f} sec'.format(time.time()-st_t))\n",
    "        loss_mean /= (X_train.size(0))\n",
    "        train_ep_loss.append(loss_mean)\n",
    "        \n",
    "        # Validation\n",
    "        st_t = time.time()\n",
    "        model.eval()\n",
    "        loss_mean_valid = 0\n",
    "        v_loss_list = []\n",
    "        \n",
    "        for i in range(X_validate.size(0)):\n",
    "            # get the images inputs\n",
    "            inputs = X_validate[i]\n",
    "            # get the original poses\n",
    "            labels = y_validate[i]\n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            # get mse loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            ls = loss.item()\n",
    "            # update loss values\n",
    "            v_loss_list.append(float(ls))\n",
    "            loss_mean_valid += float(ls)\n",
    "        \n",
    "        print('Valid take {:.1f} sec'.format(time.time()-st_t))\n",
    "        loss_mean_valid /= X_validate.size(0)\n",
    "        valid_ep_loss.append(loss_mean_valid)\n",
    "        \n",
    "        print('Epoch {}\\ntrain loss mean: {}, std: {:.2f}\\nvalid loss mean: {}, std: {:.2f}\\n'.format(ep+1, loss_mean, np.std(t_loss_list), loss_mean_valid, np.std(v_loss_list)))\n",
    "        \n",
    "    return train_ep_loss, valid_ep_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Train take 444.6 sec\n",
      "Valid take 23.4 sec\n",
      "Epoch 1\n",
      "train loss mean: 0.6260546097505457, std: 1.44\n",
      "valid loss mean: 0.5318316136597445, std: 0.92\n",
      "\n",
      "==================================================\n",
      "Train take 445.1 sec\n",
      "Valid take 20.7 sec\n",
      "Epoch 2\n",
      "train loss mean: 0.6085276102480082, std: 1.40\n",
      "valid loss mean: 0.5482245036240282, std: 0.93\n",
      "\n",
      "==================================================\n",
      "Train take 513.7 sec\n",
      "Valid take 47.7 sec\n",
      "Epoch 3\n",
      "train loss mean: 0.5928876427789029, std: 1.35\n",
      "valid loss mean: 0.5693957967633717, std: 0.89\n",
      "\n",
      "==================================================\n",
      "Train take 746.7 sec\n",
      "Valid take 35.4 sec\n",
      "Epoch 4\n",
      "train loss mean: 0.5971575640843798, std: 1.34\n",
      "valid loss mean: 0.6941229392742289, std: 0.85\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4db5a79afa8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_model_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-11b216c8c9c7>\u001b[0m in \u001b[0;36mtraining_model_v2\u001b[0;34m(model, X_train, y_train, X_validate, y_validate, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m# set next optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = training_model_v2(model, X_batch_train, y_batch_train, X_batch_validation, y_batch_validation, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(30)),train_loss, label='Training Loss')\n",
    "plt.plot(list(range(30)),valid_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('images_model/train_valid_loss_batch_dropout.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model (model, test_num, X):\n",
    "    start_time = time.time()\n",
    "    Y_output = []\n",
    "    count = 0\n",
    "    totcount = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(test_num):\n",
    "            # get the inputs\n",
    "            inputs = X[i]\n",
    "            outputs = model(inputs)\n",
    "            Y_output.append(outputs)\n",
    "\n",
    "    print (\"Time taken in Testing {0}\".format((time.time() - start_time)))\n",
    "    return torch.stack(Y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X,y = VODataLoader(datapath='dataset', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0] = X[0].view(-1,1,6,144,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = testing_model(model,X[0].size(0),X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[0].view(-1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to get accuracy\n",
    "def get_accuracy(outputs, labels, batch_size):\n",
    "    diff =0\n",
    "    for i in range(batch_size):\n",
    "        out = (outputs[i].detach().numpy())[0]\n",
    "        lab = (labels[i].detach().numpy())[0]\n",
    "\n",
    "        diff+=get_mse_diff(out,lab)\n",
    "        \n",
    "    print(\"Accuracy : \",(1 -diff/(batch_size))*100,\"%\")\n",
    "    \n",
    "def get_mse_diff(x,y):\n",
    "    diff= 0\n",
    "    for i in range(6):\n",
    "        diff += (x[i]-y[i]) ** 2\n",
    "    return diff/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(y_out, y, 497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/deepvo-train-valid2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (outputs[j].detach().numpy())[0]\n",
    "x_ori = []\n",
    "y_ori = []\n",
    "for i in range(1,len(y)):\n",
    "    out = y[i]\n",
    "    out = (out.detach().numpy())[0]\n",
    "    x_ori.append(out[0])\n",
    "    y_ori.append(out[0])    \n",
    "#     x_ori.append(x_ori[i-1] + out[0])\n",
    "#     y_ori.append(y_ori[i-1] + out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o = []\n",
    "y_o = []\n",
    "for i in range(len(y_out)):    \n",
    "    out = y_out[i]\n",
    "    out = (out.detach().numpy())[0]\n",
    "    x_o.append(out[0])\n",
    "    y_o.append(out[1])\n",
    "#     x_o.append(x_o[i-1] + out[0])\n",
    "#     y_o.append(y_o[i-1] + out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.cumsum(x_ori),np.cumsum(y_ori))\n",
    "plt.scatter(x_ori[0],y_ori[0],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.cumsum(x_o),np.cumsum(y_o))\n",
    "plt.scatter(x_o[0],y_o[0],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
