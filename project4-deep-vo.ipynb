{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 - Mc907/Mo651 - Mobile Robotics\n",
    "\n",
    "### Student:\n",
    "Luiz Eduardo Cartolano - RA: 183012\n",
    "\n",
    "### Instructor:\n",
    "Esther Luna Colombini\n",
    "\n",
    "### Github Link:\n",
    "[Project Repository](https://github.com/luizcartolano2/mc907-mobile-robotics)\n",
    "\n",
    "### Youtube Link:\n",
    "[Link to Video](https://youtu.be/uqNeEhWo0dA)\n",
    "\n",
    "### Subject of this Work:\n",
    "The general objective of this work is to implement a deep learning approach for solve the Visual Odometry problem.\n",
    "\n",
    "### Goals:\n",
    "1. Implement and evaluate a Deep VO strategy using images from the [AirSim](https://github.com/microsoft/AirSim) simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean wrong images\n",
    "\n",
    "While upload images obtained from the AirSim simulator were noted that some of them had failure, so, we have to clean this data to avoid noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "|    dataset/seq1/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq2/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq3/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq4/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq5/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq6/\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dt in ['1','2','3','4','5','6']:\n",
    "    path = 'dataset/'+'seq'+dt+'/'\n",
    "    print(\"-------------------------------------------\")\n",
    "    print('|    '+path)\n",
    "    all_images = glob.glob(path+'images'+'/*')\n",
    "    df_poses = pd.read_csv(path+'poses.csv')[['ImageFile']].values\n",
    "    for img in df_poses:\n",
    "        if not (path+'images/'+img) in all_images:\n",
    "            print('|        '+img[0])\n",
    "    print(\"-------------------------------------------\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rgb_mean(image_sequence):\n",
    "    '''\n",
    "        Compute the mean over each channel separately over a set of images.\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_sequence  :   np.ndarray\n",
    "                        Array of shape ``(N, h, w, c)`` or ``(h, w, c)``\n",
    "    '''\n",
    "    if image_sequence.ndim == 4:\n",
    "        _, h, w, c = image_sequence.shape\n",
    "    if image_sequence.ndim == 3:\n",
    "        h, w, c = image_sequence.shape\n",
    "    # compute mean separately for each channel\n",
    "    # somehow this expression is buggy, so we must do it manually\n",
    "    # mode = image_sequence.mean((0, 1, 2))\n",
    "    mean_r = image_sequence[..., 0].mean()\n",
    "    mean_g = image_sequence[..., 1].mean()\n",
    "    mean_b = image_sequence[..., 2].mean()\n",
    "    mean = np.array([mean_r, mean_g, mean_b])\n",
    "    return mean\n",
    "\n",
    "def mean_normalize(images_vector):\n",
    "    '''\n",
    "        Normalize data to the range -1 to 1\n",
    "    '''\n",
    "    out_images = []\n",
    "    \n",
    "    N = len(images_vector)\n",
    "    \n",
    "    print('|    Mean-normalizing ...')\n",
    "\n",
    "    mean_accumlator = np.zeros((3,), dtype=np.float32)\n",
    "\n",
    "    for idx in range(N):\n",
    "        img = images_vector[idx]\n",
    "        mean_accumlator += compute_rgb_mean(img)\n",
    "\n",
    "    mean_accumlator /= N\n",
    "    print(f'|    Mean: {mean_accumlator}')\n",
    "    \n",
    "    for idx in range(N):\n",
    "        img = images_vector[idx]\n",
    "        out_images.append(img - mean_accumlator)\n",
    "    \n",
    "    print('|    Done')\n",
    "    \n",
    "    return out_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path,img_size=(256,144)):\n",
    "    \"\"\"\n",
    "        Function to read an image from a given path.\n",
    "        \n",
    "        :param: path - image path\n",
    "        :param: img_size - image size\n",
    "        \n",
    "        :return: img - numpy array with the images pixels (converted to grayscale and normalized)\n",
    "        \n",
    "    \"\"\"\n",
    "    # read image from path\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    # normalize image pixels\n",
    "    img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(img_dir, img_size=(256,144)):\n",
    "\n",
    "    \"\"\"\n",
    "        Function to coordinate the load of all the images that are going to be used.\n",
    "        \n",
    "        :param: img_dir - path to the directory containing the images\n",
    "        :param: img_size - image size\n",
    "        \n",
    "        :return: images_set - numpy array with all images at the set\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print (\"|    Loading images from: \", img_dir)\n",
    "    \n",
    "    # loop to read all the images of the directory\n",
    "    images = [get_image(img,img_size) for img in glob.glob(img_dir+'/*')]\n",
    "    \n",
    "    # normalize images\n",
    "    images = mean_normalize(images)\n",
    "    \n",
    "    #resemble images as RGB\n",
    "    images = [img[:, :, (2, 1, 0)] for img in images]\n",
    "    \n",
    "    #Transpose the image that channels num. as first dimension\n",
    "    images = [np.transpose(img,(2,0,1)) for img in images]\n",
    "    images = [torch.from_numpy(img) for img in images]\n",
    "    \n",
    "    #stack per 2 images\n",
    "    images = [np.concatenate((images[k],images[k+1]),axis = 0) for k in range(len(images)-1)]\n",
    "        \n",
    "    print(\"|    Images count : \",len(images))\n",
    "\n",
    "    # reshape the array of all images\n",
    "    images_set = np.stack(images,axis=0)\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "    return images_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions are used to the Kitti Dataset poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRotationMatrix(R):\n",
    "    \"\"\" \n",
    "        Checks if a matrix is a valid rotation matrix referred from \n",
    "        https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "        \n",
    "        :param: R - rotation matrix\n",
    "        \n",
    "        :return: True or False\n",
    "        \n",
    "    \"\"\"\n",
    "    # calc the transpose\n",
    "    Rt = np.transpose(R)\n",
    "\n",
    "    # check identity\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    \n",
    "    return n < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\" \n",
    "        Calculates rotation matrix to euler angles\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles\n",
    "        \n",
    "        :param: R - rotation matrix\n",
    "        \n",
    "        :return: rotation matrix for Euler angles\n",
    "    \"\"\"\n",
    "    assert(isRotationMatrix(R))\n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrices(all_poses):\n",
    "    \"\"\"\n",
    "        Function to extract matrices from poses\n",
    "        \n",
    "        :param: all_poses - list with all poses from the sequence\n",
    "        \n",
    "        :return: all_matrices - list with all matrices obtained from the poses\n",
    "    \"\"\"\n",
    "    all_matrices = []\n",
    "    for i in range(len(all_poses)):\n",
    "        #print(\"I: \",i)\n",
    "        j = all_poses[i]\n",
    "        #print(\"J:   \",j)\n",
    "        p = np.array([j[3], j[7], j[11]])\n",
    "        #print(\"P:   \", p)\n",
    "        R = np.array([[j[0],j[1],j[2]],\n",
    "                      [j[4],j[5],j[6]],\n",
    "                      [j[8],j[9],j[10]]\n",
    "                     ])\n",
    "        #print(\"R:   \", R)\n",
    "        angles = rotationMatrixToEulerAngles(R)\n",
    "        #print(\"Angles: \",angles)\n",
    "        matrix = np.concatenate((p,angles))\n",
    "        #print(\"MATRIX: \", matrix)\n",
    "        all_matrices.append(matrix)\n",
    "    return all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kitti_images(pose_file):\n",
    "    poses = []\n",
    "    poses_set = []\n",
    "\n",
    "    with open(pose_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose = np.fromstring(line, dtype=float, sep=' ')\n",
    "            poses.append(pose)\n",
    "    \n",
    "    poses = getMatrices(poses)\n",
    "    pose1 = poses[0]\n",
    "    for i in range(len(poses)-1):\n",
    "        pose2 = poses[i+1]\n",
    "        finalpose = np.zeros(pose1)\n",
    "        poses_set.append(finalpose)\n",
    "\n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions are used for the poses obtained from the AirSim simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pi_to_poses(pose):\n",
    "    '''Add Pi to every pose angle.'''\n",
    "    pose += np.pi\n",
    "    \n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_to_euler_angles(quat_matrix):\n",
    "    # create a scipy object from the quaternion angles\n",
    "    rot_mat = R.from_quat(quat_matrix)\n",
    "    # convert the quaternion to euler (in degrees)\n",
    "    euler_mat = rot_mat.as_euler('yxz', degrees=False)\n",
    "    # convert from (-pi,pi) to (0,2pi)\n",
    "    euler_convert = add_pi_to_poses(euler_mat)\n",
    "    \n",
    "    return euler_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_airsim_pose(pose_file):\n",
    "    \n",
    "    poses = []\n",
    "    poses_set = []\n",
    "    \n",
    "    df_poses = pd.read_csv(pose_file)\n",
    "    for index, row in df_poses.iterrows():\n",
    "        # get the (x,y,z) positions of the camera\n",
    "        position = np.array([row['POS_X'],row['POS_Y'],row['POS_Z']])\n",
    "        # get the quaternions angles of the camera\n",
    "        quat_matrix = np.array([row['Q_X'],row['Q_Y'], row['Q_Z'],row['Q_W']])\n",
    "        # call the func that convert the quaternions to euler angles\n",
    "        euler_matrix = quat_to_euler_angles(quat_matrix)\n",
    "        # concatenate both position(x,y,z) and euler angles\n",
    "        poses.append(np.concatenate((position,euler_matrix)))\n",
    "    \n",
    "    # make the first pose as start position\n",
    "    pose1 = poses[0]\n",
    "    for i in range(len(poses)):\n",
    "        pose2 = poses[i]\n",
    "        pose_diff = np.subtract(pose2, pose1)\n",
    "        pose_diff[4:] = np.arctan2(np.sin(pose_diff[4:]), np.cos(pose_diff[4:]))\n",
    "        \n",
    "        poses[i] = pose_diff\n",
    "    \n",
    "    # get the desloc between two poses\n",
    "    for i in range(len(poses)-1):\n",
    "        pose1 = poses[i]\n",
    "        pose2 = poses[i+1]\n",
    "\n",
    "        pose_diff = np.subtract(pose2, pose1)\n",
    "        pose_diff[4:] = np.arctan2(np.sin(pose_diff[4:]), np.cos(pose_diff[4:]))\n",
    "\n",
    "        poses_set.append(pose_diff)\n",
    "        \n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poses(pose_file, pose_format='airsim'):\n",
    "    \"\"\"\n",
    "        Function to load the image poses.\n",
    "        \n",
    "        :param: pose_file - path to the pose file\n",
    "        :param: pose_format - where the pose were obtained from (AirSim, VREP, Kitti, etc...)\n",
    "        \n",
    "        :return: pose_set - set of the poses for the sequence\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print (\"|    Pose from: \",pose_file)\n",
    "    \n",
    "    if pose_format.lower() == 'kitti':\n",
    "        poses_set = load_kitti_images(pose_file)\n",
    "    elif pose_format.lower() == 'airsim':\n",
    "        poses_set = load_airsim_pose(pose_file)\n",
    "        \n",
    "    print(\"|        Poses count: \",len(poses_set))\n",
    "    print(\"----------------------------------------------------------------------\")    \n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "Function that acquire all data that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VODataLoader(datapath,img_size=(256,144), test=False):\n",
    "    if test:\n",
    "        sequences = ['3']\n",
    "    else:\n",
    "        sequences = ['1','2','4','5','6']\n",
    "        \n",
    "    images_set = []\n",
    "    odometry_set = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        dir_path = os.path.join(datapath,'seq'+sequence)\n",
    "        image_path = os.path.join(dir_path,'images')\n",
    "        pose_path = os.path.join(dir_path,'poses.csv')\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        print(\"|Load from: \", dir_path)\n",
    "        images_set.append(torch.FloatTensor(load_images(image_path,img_size)))\n",
    "        odometry_set.append(torch.FloatTensor(load_poses(pose_path, 'AirSim')))\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"|   Total Images: \", len(images_set))\n",
    "    print(\"|   Total Odometry: \", len(odometry_set))\n",
    "    print(\"---------------------------------------------------\")    \n",
    "    return images_set, odometry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq1\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq1/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.3115534  0.29183614 0.27286035]\n",
      "|    Done\n",
      "|    Images count :  326\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq1/poses.csv\n",
      "|        Poses count:  326\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq2\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq2/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.2615339  0.24993458 0.2350733 ]\n",
      "|    Done\n",
      "|    Images count :  283\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq2/poses.csv\n",
      "|        Poses count:  283\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq4\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq4/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.34079915 0.32100374 0.28959265]\n",
      "|    Done\n",
      "|    Images count :  368\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq4/poses.csv\n",
      "|        Poses count:  368\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq5\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq5/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.13811225 0.20660812 0.3470334 ]\n",
      "|    Done\n",
      "|    Images count :  121\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq5/poses.csv\n",
      "|        Poses count:  121\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq6\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq6/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.59857106 0.6014284  0.56355304]\n",
      "|    Done\n",
      "|    Images count :  208\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq6/poses.csv\n",
      "|        Poses count:  208\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "|   Total Images:  5\n",
      "|   Total Odometry:  5\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X,y = VODataLoader(datapath='dataset', test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting lists containing tensors to tensors as per the batchsize (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [item for x in X for item in x]\n",
    "Y_train = [item for a in y for item in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info about the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Details of X :\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "1306\n",
      "torch.Size([6, 144, 256])\n",
      "---------------------------------\n",
      "Details of y :\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "1306\n",
      "torch.Size([6])\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"Details of X :\")\n",
    "print(type(X_train)) \n",
    "print(type(X_train[0]))\n",
    "print(len(X_train)) \n",
    "print(X_train[0].size())\n",
    "print(\"---------------------------------\")\n",
    "print(\"Details of y :\")\n",
    "print(type(Y_train))\n",
    "print(type(Y_train[0]))\n",
    "print(len(Y_train))\n",
    "print(Y_train[0].size())\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = torch.stack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stack = torch.stack(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = X_stack.view(-1,1,6,144,256)\n",
    "y_batch = y_stack.view(-1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([1306, 1, 6, 144, 256])\n",
      "Details of y :\n",
      "torch.Size([1306, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Details of X :\")\n",
    "print(X_batch.size())\n",
    "print(\"Details of y :\")\n",
    "print(y_batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .2\n",
    "dataset_size = len(X_batch)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch_train = X_batch[split:]\n",
    "y_batch_train = y_batch[split:]\n",
    "X_batch_validation = X_batch[:split]\n",
    "y_batch_validation = y_batch[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining DeepVO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepVONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepVONet, self).__init__()\n",
    "        # CNN\n",
    "        # convolutional layer 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False), #6 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2), \n",
    "        )\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 3_1        \n",
    "        self.conv3_1 = nn.Sequential(\n",
    "            nn.Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 4_1\n",
    "        self.conv4_1 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 5\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 5_1\n",
    "        self.conv5_1 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 6\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d (512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        # RNN\n",
    "        self.lstm1 = nn.LSTMCell(2*6*1024, 100)\n",
    "        self.lstm2 = nn.LSTMCell(100, 100)\n",
    "        self.fc = nn.Linear(in_features=100, out_features=6)\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def reset_hidden_states(self, size=1, zero=True):\n",
    "        if zero == True:\n",
    "            self.hx1 = Variable(torch.zeros(size, 100))\n",
    "            self.cx1 = Variable(torch.zeros(size, 100))\n",
    "            self.hx2 = Variable(torch.zeros(size, 100))\n",
    "            self.cx2 = Variable(torch.zeros(size, 100))\n",
    "        else:\n",
    "            self.hx1 = Variable(self.hx1.data)\n",
    "            self.cx1 = Variable(self.cx1.data)\n",
    "            self.hx2 = Variable(self.hx2.data)\n",
    "            self.cx2 = Variable(self.cx2.data)\n",
    "\n",
    "        if next(self.parameters()).is_cuda == True:\n",
    "            self.hx1 = self.hx1.cuda()\n",
    "            self.cx1 = self.cx1.cuda()\n",
    "            self.hx2 = self.hx2.cuda()\n",
    "            self.cx2 = self.cx2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = x.view(x.size(0), 2 * 6 * 1024)\n",
    "        \n",
    "        self.hx1, self.cx1 = self.lstm1(x, (self.hx1, self.cx1))\n",
    "        x = self.hx1\n",
    "        self.hx2, self.cx2 = self.lstm2(x, (self.hx2, self.cx2))\n",
    "        x = self.hx2\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVONet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv3_1): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv4_1): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv5_1): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (lstm1): LSTMCell(12288, 100)\n",
      "  (lstm2): LSTMCell(100, 100)\n",
      "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = DeepVONet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss and optimizer to be used \n",
    "criterion = torch.nn.MSELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, weight_decay=0.5)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the pretrained weight of FlowNet ( CNN part )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained = torch.load('flownets_EPE1.951.pth.tar',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dict['conv1.weight'] = pre_trained['state_dict']['conv1.0.weight']\n",
    "update_dict['conv2.weight'] = pre_trained['state_dict']['conv2.0.weight']\n",
    "update_dict['conv3.weight'] = pre_trained['state_dict']['conv3.0.weight']\n",
    "update_dict['conv4.weight'] = pre_trained['state_dict']['conv4.0.weight']\n",
    "update_dict['conv4_1.weight'] = pre_trained['state_dict']['conv4_1.0.weight']\n",
    "update_dict['conv5.weight'] = pre_trained['state_dict']['conv5.0.weight']\n",
    "update_dict['conv5_1.weight'] = pre_trained['state_dict']['conv5_1.0.weight']\n",
    "update_dict['conv6.weight'] = pre_trained['state_dict']['conv6.0.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(update_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, train_num, X, y, epoch_num=25):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        for i in range(train_num):\n",
    "            print(\"    Train num :\", i+1)\n",
    "            inputs = X[i]\n",
    "            print(\"        Input Size: {}\".format(inputs.size()))\n",
    "            labels = y[i]\n",
    "            print(\"        Labels: \",labels)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            print(\"        Outputs: \",outputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('    Epoch : %d Loss: %.3f' %(epoch+1, running_loss/train_num))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    print (\"Time taken in Training {0}\".format((time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_v2(model, X_train, y_train, X_validate, y_validate, num_epochs):\n",
    "    # start model train mode\n",
    "    train_ep_loss = []\n",
    "    valid_ep_loss = []\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "        st_t = time.time()\n",
    "        print('='*50)\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        \n",
    "        loss_mean = 0\n",
    "        t_loss_list = []\n",
    "        \n",
    "        for i in range(X_train.size(0)):\n",
    "            # get the images inputs\n",
    "            inputs = X_train[i]\n",
    "            # get the original poses\n",
    "            labels = y_train[i]\n",
    "            # zero optimizer\n",
    "            optimizer.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "            \n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            # get mse loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            ls = loss.item()\n",
    "            loss.backward()\n",
    "            # set next optimizer step\n",
    "            optimizer.step()\n",
    "            # append loss\n",
    "            t_loss_list.append(float(ls))\n",
    "            # update loss\n",
    "            loss_mean += float(ls)\n",
    "        \n",
    "        print('Train take {:.1f} sec'.format(time.time()-st_t))\n",
    "        loss_mean /= (X_train.size(0))\n",
    "        train_ep_loss.append(loss_mean)\n",
    "        \n",
    "        # Validation\n",
    "        st_t = time.time()\n",
    "        model.eval()\n",
    "        loss_mean_valid = 0\n",
    "        v_loss_list = []\n",
    "        \n",
    "        for i in range(X_validate.size(0)):\n",
    "            # get the images inputs\n",
    "            inputs = X_validate[i]\n",
    "            # get the original poses\n",
    "            labels = y_validate[i]\n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            # get mse loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            ls = loss.item()\n",
    "            # update loss values\n",
    "            v_loss_list.append(float(ls))\n",
    "            loss_mean_valid += float(ls)\n",
    "        \n",
    "        print('Valid take {:.1f} sec'.format(time.time()-st_t))\n",
    "        loss_mean_valid /= X_validate.size(0)\n",
    "        valid_ep_loss.append(loss_mean_valid)\n",
    "        \n",
    "        print('Epoch {}\\ntrain loss mean: {}, std: {:.2f}\\nvalid loss mean: {}, std: {:.2f}\\n'.format(ep+1, loss_mean, np.std(t_loss_list), loss_mean_valid, np.std(v_loss_list)))\n",
    "        \n",
    "    return train_ep_loss, valid_ep_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Train take 566.0 sec\n",
      "Valid take 20.6 sec\n",
      "Epoch 1\n",
      "train loss mean: 0.47443677749157254, std: 1.05\n",
      "valid loss mean: 0.6021302036189036, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 428.7 sec\n",
      "Valid take 27.2 sec\n",
      "Epoch 2\n",
      "train loss mean: 0.46476366728021756, std: 1.03\n",
      "valid loss mean: 0.6465458333103336, std: 1.16\n",
      "\n",
      "==================================================\n",
      "Train take 437.4 sec\n",
      "Valid take 19.5 sec\n",
      "Epoch 3\n",
      "train loss mean: 0.4659245623534648, std: 1.05\n",
      "valid loss mean: 0.6530070472869958, std: 1.16\n",
      "\n",
      "==================================================\n",
      "Train take 426.9 sec\n",
      "Valid take 19.8 sec\n",
      "Epoch 4\n",
      "train loss mean: 0.456445613158527, std: 1.01\n",
      "valid loss mean: 0.6038745464168318, std: 1.11\n",
      "\n",
      "==================================================\n",
      "Train take 418.3 sec\n",
      "Valid take 22.5 sec\n",
      "Epoch 5\n",
      "train loss mean: 0.4547543018580911, std: 1.03\n",
      "valid loss mean: 0.5856295773354484, std: 1.07\n",
      "\n",
      "==================================================\n",
      "Train take 425.1 sec\n",
      "Valid take 23.0 sec\n",
      "Epoch 6\n",
      "train loss mean: 0.44480006992867055, std: 1.03\n",
      "valid loss mean: 0.5824676825356281, std: 1.08\n",
      "\n",
      "==================================================\n",
      "Train take 427.1 sec\n",
      "Valid take 19.5 sec\n",
      "Epoch 7\n",
      "train loss mean: 0.44278786766128925, std: 1.01\n",
      "valid loss mean: 0.5936873180812253, std: 0.96\n",
      "\n",
      "==================================================\n",
      "Train take 423.3 sec\n",
      "Valid take 19.3 sec\n",
      "Epoch 8\n",
      "train loss mean: 0.4387302398761711, std: 1.01\n",
      "valid loss mean: 0.6068884764130716, std: 1.10\n",
      "\n",
      "==================================================\n",
      "Train take 427.0 sec\n",
      "Valid take 19.9 sec\n",
      "Epoch 9\n",
      "train loss mean: 0.4351016018177749, std: 1.01\n",
      "valid loss mean: 0.5663707224048685, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 419.6 sec\n",
      "Valid take 19.5 sec\n",
      "Epoch 10\n",
      "train loss mean: 0.4288066581712922, std: 1.00\n",
      "valid loss mean: 0.6218348327057144, std: 1.12\n",
      "\n",
      "==================================================\n",
      "Train take 425.2 sec\n",
      "Valid take 20.7 sec\n",
      "Epoch 11\n",
      "train loss mean: 0.4247168340075496, std: 0.99\n",
      "valid loss mean: 0.5875970757030258, std: 1.07\n",
      "\n",
      "==================================================\n",
      "Train take 414.4 sec\n",
      "Valid take 19.7 sec\n",
      "Epoch 12\n",
      "train loss mean: 0.42432371030977156, std: 1.01\n",
      "valid loss mean: 0.5992217933147133, std: 1.09\n",
      "\n",
      "==================================================\n",
      "Train take 417.6 sec\n",
      "Valid take 20.1 sec\n",
      "Epoch 13\n",
      "train loss mean: 0.4134766020642763, std: 0.99\n",
      "valid loss mean: 0.6137375898045484, std: 1.10\n",
      "\n",
      "==================================================\n",
      "Train take 426.8 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 14\n",
      "train loss mean: 0.40731404383146563, std: 0.98\n",
      "valid loss mean: 0.5777874974296624, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 423.1 sec\n",
      "Valid take 19.5 sec\n",
      "Epoch 15\n",
      "train loss mean: 0.4044428369212499, std: 0.97\n",
      "valid loss mean: 0.603490274974013, std: 1.09\n",
      "\n",
      "==================================================\n",
      "Train take 416.6 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 16\n",
      "train loss mean: 0.397740295085372, std: 0.96\n",
      "valid loss mean: 0.5793685814788586, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 428.2 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 17\n",
      "train loss mean: 0.396175754891762, std: 0.97\n",
      "valid loss mean: 0.5943937434741721, std: 0.98\n",
      "\n",
      "==================================================\n",
      "Train take 421.9 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 18\n",
      "train loss mean: 0.39235335182884273, std: 0.96\n",
      "valid loss mean: 0.577062311965189, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 424.8 sec\n",
      "Valid take 20.3 sec\n",
      "Epoch 19\n",
      "train loss mean: 0.383386821205935, std: 0.95\n",
      "valid loss mean: 0.6102788973509573, std: 1.09\n",
      "\n",
      "==================================================\n",
      "Train take 432.7 sec\n",
      "Valid take 20.8 sec\n",
      "Epoch 20\n",
      "train loss mean: 0.381531709092296, std: 0.95\n",
      "valid loss mean: 0.6073956201245502, std: 1.10\n",
      "\n",
      "==================================================\n",
      "Train take 423.7 sec\n",
      "Valid take 19.7 sec\n",
      "Epoch 21\n",
      "train loss mean: 0.3775982576980379, std: 0.94\n",
      "valid loss mean: 0.6091603258200047, std: 1.12\n",
      "\n",
      "==================================================\n",
      "Train take 429.4 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 22\n",
      "train loss mean: 0.3757380001340507, std: 0.94\n",
      "valid loss mean: 0.5743993573797103, std: 1.08\n",
      "\n",
      "==================================================\n",
      "Train take 424.4 sec\n",
      "Valid take 19.8 sec\n",
      "Epoch 23\n",
      "train loss mean: 0.37301143754238003, std: 0.93\n",
      "valid loss mean: 0.5693295684257715, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 444.7 sec\n",
      "Valid take 20.1 sec\n",
      "Epoch 24\n",
      "train loss mean: 0.36527729362868216, std: 0.94\n",
      "valid loss mean: 0.627952253333582, std: 1.12\n",
      "\n",
      "==================================================\n",
      "Train take 422.9 sec\n",
      "Valid take 19.7 sec\n",
      "Epoch 25\n",
      "train loss mean: 0.36735430871042907, std: 0.92\n",
      "valid loss mean: 0.5559751089016454, std: 1.00\n",
      "\n",
      "==================================================\n",
      "Train take 423.6 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 26\n",
      "train loss mean: 0.35623870498492965, std: 0.91\n",
      "valid loss mean: 0.5701804789714515, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 426.6 sec\n",
      "Valid take 20.7 sec\n",
      "Epoch 27\n",
      "train loss mean: 0.35982285624987703, std: 0.92\n",
      "valid loss mean: 0.549131704281152, std: 1.03\n",
      "\n",
      "==================================================\n",
      "Train take 413.7 sec\n",
      "Valid take 23.1 sec\n",
      "Epoch 28\n",
      "train loss mean: 0.35342753790494424, std: 0.92\n",
      "valid loss mean: 0.57819879769394, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 421.2 sec\n",
      "Valid take 20.2 sec\n",
      "Epoch 29\n",
      "train loss mean: 0.3527585741048041, std: 0.91\n",
      "valid loss mean: 0.5484025718044789, std: 1.02\n",
      "\n",
      "==================================================\n",
      "Train take 420.0 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 30\n",
      "train loss mean: 0.3470189865134906, std: 0.91\n",
      "valid loss mean: 0.5486228557417733, std: 1.00\n",
      "\n",
      "==================================================\n",
      "Train take 421.2 sec\n",
      "Valid take 19.3 sec\n",
      "Epoch 31\n",
      "train loss mean: 0.35116576237080294, std: 0.92\n",
      "valid loss mean: 0.5295904730471168, std: 0.97\n",
      "\n",
      "==================================================\n",
      "Train take 450.9 sec\n",
      "Valid take 21.4 sec\n",
      "Epoch 32\n",
      "train loss mean: 0.3473047324338745, std: 0.91\n",
      "valid loss mean: 0.5555432171719613, std: 1.00\n",
      "\n",
      "==================================================\n",
      "Train take 429.4 sec\n",
      "Valid take 20.3 sec\n",
      "Epoch 33\n",
      "train loss mean: 0.347339893560177, std: 0.90\n",
      "valid loss mean: 0.5932002266069653, std: 1.09\n",
      "\n",
      "==================================================\n",
      "Train take 419.7 sec\n",
      "Valid take 20.1 sec\n",
      "Epoch 34\n",
      "train loss mean: 0.3460520136119262, std: 0.90\n",
      "valid loss mean: 0.593149288746589, std: 1.09\n",
      "\n",
      "==================================================\n",
      "Train take 423.1 sec\n",
      "Valid take 20.5 sec\n",
      "Epoch 35\n",
      "train loss mean: 0.3349518817905089, std: 0.89\n",
      "valid loss mean: 0.5559293217118473, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 417.1 sec\n",
      "Valid take 20.3 sec\n",
      "Epoch 36\n",
      "train loss mean: 0.33558225502452477, std: 0.90\n",
      "valid loss mean: 0.568538200897004, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 414.4 sec\n",
      "Valid take 19.7 sec\n",
      "Epoch 37\n",
      "train loss mean: 0.3358992702138981, std: 0.90\n",
      "valid loss mean: 0.5557728318775865, std: 1.03\n",
      "\n",
      "==================================================\n",
      "Train take 421.4 sec\n",
      "Valid take 19.3 sec\n",
      "Epoch 38\n",
      "train loss mean: 0.3360880394319449, std: 0.91\n",
      "valid loss mean: 0.5540333136111513, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 419.7 sec\n",
      "Valid take 19.6 sec\n",
      "Epoch 39\n",
      "train loss mean: 0.3297024188603472, std: 0.90\n",
      "valid loss mean: 0.5957979872052012, std: 1.10\n",
      "\n",
      "==================================================\n",
      "Train take 422.7 sec\n",
      "Valid take 20.7 sec\n",
      "Epoch 40\n",
      "train loss mean: 0.33369174376455957, std: 0.89\n",
      "valid loss mean: 0.5636372934898426, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 422.0 sec\n",
      "Valid take 20.6 sec\n",
      "Epoch 41\n",
      "train loss mean: 0.3294784416451841, std: 0.88\n",
      "valid loss mean: 0.5350981218895564, std: 0.99\n",
      "\n",
      "==================================================\n",
      "Train take 430.8 sec\n",
      "Valid take 20.7 sec\n",
      "Epoch 42\n",
      "train loss mean: 0.3262393029684755, std: 0.89\n",
      "valid loss mean: 0.5516988368904739, std: 1.04\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train take 423.0 sec\n",
      "Valid take 21.2 sec\n",
      "Epoch 43\n",
      "train loss mean: 0.32340117594907863, std: 0.88\n",
      "valid loss mean: 0.546704500027793, std: 1.04\n",
      "\n",
      "==================================================\n",
      "Train take 411.6 sec\n",
      "Valid take 20.9 sec\n",
      "Epoch 44\n",
      "train loss mean: 0.32990862470533494, std: 0.89\n",
      "valid loss mean: 0.5784205409530478, std: 1.07\n",
      "\n",
      "==================================================\n",
      "Train take 411.6 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 45\n",
      "train loss mean: 0.3280445659134045, std: 0.87\n",
      "valid loss mean: 0.570512072898633, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 424.3 sec\n",
      "Valid take 19.6 sec\n",
      "Epoch 46\n",
      "train loss mean: 0.3161229843556233, std: 0.86\n",
      "valid loss mean: 0.5445432503448114, std: 1.04\n",
      "\n",
      "==================================================\n",
      "Train take 426.4 sec\n",
      "Valid take 20.2 sec\n",
      "Epoch 47\n",
      "train loss mean: 0.31484377207003345, std: 0.88\n",
      "valid loss mean: 0.5453577022863484, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 430.3 sec\n",
      "Valid take 20.2 sec\n",
      "Epoch 48\n",
      "train loss mean: 0.3168070090603352, std: 0.87\n",
      "valid loss mean: 0.5484781471563689, std: 1.02\n",
      "\n",
      "==================================================\n",
      "Train take 422.7 sec\n",
      "Valid take 19.9 sec\n",
      "Epoch 49\n",
      "train loss mean: 0.32098935847575877, std: 0.88\n",
      "valid loss mean: 0.5602809202784166, std: 1.07\n",
      "\n",
      "==================================================\n",
      "Train take 427.1 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 50\n",
      "train loss mean: 0.31630887061965474, std: 0.88\n",
      "valid loss mean: 0.5345880927180584, std: 1.02\n",
      "\n",
      "==================================================\n",
      "Train take 424.6 sec\n",
      "Valid take 19.8 sec\n",
      "Epoch 51\n",
      "train loss mean: 0.3130004142556224, std: 0.87\n",
      "valid loss mean: 0.5252697452170403, std: 1.02\n",
      "\n",
      "==================================================\n",
      "Train take 430.5 sec\n",
      "Valid take 20.8 sec\n",
      "Epoch 52\n",
      "train loss mean: 0.310371864716142, std: 0.87\n",
      "valid loss mean: 0.5776823343871023, std: 1.10\n",
      "\n",
      "==================================================\n",
      "Train take 424.7 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 53\n",
      "train loss mean: 0.3118904143841447, std: 0.86\n",
      "valid loss mean: 0.5535971541006753, std: 1.07\n",
      "\n",
      "==================================================\n",
      "Train take 426.4 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 54\n",
      "train loss mean: 0.3092760483203339, std: 0.86\n",
      "valid loss mean: 0.5367589291627608, std: 1.02\n",
      "\n",
      "==================================================\n",
      "Train take 423.3 sec\n",
      "Valid take 21.8 sec\n",
      "Epoch 55\n",
      "train loss mean: 0.3175331745253966, std: 0.88\n",
      "valid loss mean: 0.5376755390690918, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 464.8 sec\n",
      "Valid take 21.3 sec\n",
      "Epoch 56\n",
      "train loss mean: 0.31326526571961977, std: 0.87\n",
      "valid loss mean: 0.5516284405891324, std: 1.07\n",
      "\n",
      "==================================================\n",
      "Train take 443.3 sec\n",
      "Valid take 20.9 sec\n",
      "Epoch 57\n",
      "train loss mean: 0.31091003382496196, std: 0.86\n",
      "valid loss mean: 0.5624236424434154, std: 1.08\n",
      "\n",
      "==================================================\n",
      "Train take 416.2 sec\n",
      "Valid take 19.8 sec\n",
      "Epoch 58\n",
      "train loss mean: 0.30652821214204545, std: 0.85\n",
      "valid loss mean: 0.5518045712617436, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 411.9 sec\n",
      "Valid take 19.6 sec\n",
      "Epoch 59\n",
      "train loss mean: 0.3089773942520073, std: 0.86\n",
      "valid loss mean: 0.5329163996529519, std: 1.02\n",
      "\n",
      "==================================================\n",
      "Train take 426.0 sec\n",
      "Valid take 20.9 sec\n",
      "Epoch 60\n",
      "train loss mean: 0.304265290673259, std: 0.86\n",
      "valid loss mean: 0.5338646408074118, std: 1.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = training_model_v2(model, X_batch_train, y_batch_train, X_batch_validation, y_batch_validation, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(75)),train_loss, label='Training Loss')\n",
    "plt.plot(list(range(75)),valid_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('images_model/train_batch_dropout_90.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model (model, test_num, X):\n",
    "    start_time = time.time()\n",
    "    Y_output = []\n",
    "    count = 0\n",
    "    totcount = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(test_num):\n",
    "            # get the inputs\n",
    "            inputs = X[i]\n",
    "            outputs = model(inputs)\n",
    "            Y_output.append(outputs)\n",
    "\n",
    "    print (\"Time taken in Testing {0}\".format((time.time() - start_time)))\n",
    "    return torch.stack(Y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X,y = VODataLoader(datapath='dataset', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0] = X[0].view(-1,1,6,144,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = testing_model(model,X[0].size(0),X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[0].view(-1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to get accuracy\n",
    "def get_accuracy(outputs, labels, batch_size):\n",
    "    diff =0\n",
    "    for i in range(batch_size):\n",
    "        out = (outputs[i].detach().numpy())[0]\n",
    "        lab = (labels[i].detach().numpy())[0]\n",
    "\n",
    "        diff+=get_mse_diff(out,lab)\n",
    "        \n",
    "    print(\"Accuracy : \",(1 -diff/(batch_size))*100,\"%\")\n",
    "    \n",
    "def get_mse_diff(x,y):\n",
    "    diff= 0\n",
    "    for i in range(6):\n",
    "        diff += (x[i]-y[i]) ** 2\n",
    "    return diff/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(y_out, y, 497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/deepvo-dropout.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (outputs[j].detach().numpy())[0]\n",
    "x_ori = []\n",
    "y_ori = []\n",
    "for i in range(1,len(y)):\n",
    "    out = y[i]\n",
    "    out = (out.detach().numpy())[0]\n",
    "    x_ori.append(out[0])\n",
    "    y_ori.append(out[0])    \n",
    "#     x_ori.append(x_ori[i-1] + out[0])\n",
    "#     y_ori.append(y_ori[i-1] + out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o = []\n",
    "y_o = []\n",
    "for i in range(len(y_out)):    \n",
    "    out = y_out[i]\n",
    "    out = (out.detach().numpy())[0]\n",
    "    x_o.append(out[0])\n",
    "    y_o.append(out[1])\n",
    "#     x_o.append(x_o[i-1] + out[0])\n",
    "#     y_o.append(y_o[i-1] + out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.cumsum(x_ori),np.cumsum(y_ori))\n",
    "plt.scatter(x_ori[0],y_ori[0],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.cumsum(x_o),np.cumsum(y_o))\n",
    "plt.scatter(x_o[0],y_o[0],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
