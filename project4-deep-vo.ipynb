{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 - Mc907/Mo651 - Mobile Robotics\n",
    "\n",
    "### Student:\n",
    "Luiz Eduardo Cartolano - RA: 183012\n",
    "\n",
    "### Instructor:\n",
    "Esther Luna Colombini\n",
    "\n",
    "### Github Link:\n",
    "[Project Repository](https://github.com/luizcartolano2/mc907-mobile-robotics)\n",
    "\n",
    "### Youtube Link:\n",
    "[Link to Video](https://youtu.be/uqNeEhWo0dA)\n",
    "\n",
    "### Subject of this Work:\n",
    "The general objective of this work is to implement a deep learning approach for solve the Visual Odometry problem.\n",
    "\n",
    "### Goals:\n",
    "1. Implement and evaluate a Deep VO strategy using images from the [AirSim](https://github.com/microsoft/AirSim) simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean wrong images\n",
    "\n",
    "While upload images obtained from the AirSim simulator were noted that some of them had failure, so, we have to clean this data to avoid noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "|    dataset/seq1/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq2/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq3/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq4/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq5/\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "|    dataset/seq6/\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dt in ['1','2','3','4','5','6']:\n",
    "    path = 'dataset/'+'seq'+dt+'/'\n",
    "    print(\"-------------------------------------------\")\n",
    "    print('|    '+path)\n",
    "    all_images = glob.glob(path+'images'+'/*')\n",
    "    df_poses = pd.read_csv(path+'poses.csv')[['ImageFile']].values\n",
    "    for img in df_poses:\n",
    "        if not (path+'images/'+img) in all_images:\n",
    "            print('|        '+img[0])\n",
    "    print(\"-------------------------------------------\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rgb_mean(image_sequence):\n",
    "    '''\n",
    "        Compute the mean over each channel separately over a set of images.\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_sequence  :   np.ndarray\n",
    "                        Array of shape ``(N, h, w, c)`` or ``(h, w, c)``\n",
    "    '''\n",
    "    if image_sequence.ndim == 4:\n",
    "        _, h, w, c = image_sequence.shape\n",
    "    if image_sequence.ndim == 3:\n",
    "        h, w, c = image_sequence.shape\n",
    "    # compute mean separately for each channel\n",
    "    # somehow this expression is buggy, so we must do it manually\n",
    "    # mode = image_sequence.mean((0, 1, 2))\n",
    "    mean_r = image_sequence[..., 0].mean()\n",
    "    mean_g = image_sequence[..., 1].mean()\n",
    "    mean_b = image_sequence[..., 2].mean()\n",
    "    mean = np.array([mean_r, mean_g, mean_b])\n",
    "    return mean\n",
    "\n",
    "def mean_normalize(images_vector):\n",
    "    '''\n",
    "        Normalize data to the range -1 to 1\n",
    "    '''\n",
    "    out_images = []\n",
    "    \n",
    "    N = len(images_vector)\n",
    "    \n",
    "    print('|    Mean-normalizing ...')\n",
    "\n",
    "    mean_accumlator = np.zeros((3,), dtype=np.float32)\n",
    "\n",
    "    for idx in range(N):\n",
    "        img = images_vector[idx]\n",
    "        mean_accumlator += compute_rgb_mean(img)\n",
    "\n",
    "    mean_accumlator /= N\n",
    "    print(f'|    Mean: {mean_accumlator}')\n",
    "    \n",
    "    for idx in range(N):\n",
    "        img = images_vector[idx]\n",
    "        out_images.append(img - mean_accumlator)\n",
    "    \n",
    "    print('|    Done')\n",
    "    \n",
    "    return out_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path,img_size=(256,144)):\n",
    "    \"\"\"\n",
    "        Function to read an image from a given path.\n",
    "        \n",
    "        :param: path - image path\n",
    "        :param: img_size - image size\n",
    "        \n",
    "        :return: img - numpy array with the images pixels (converted to grayscale and normalized)\n",
    "        \n",
    "    \"\"\"\n",
    "    # read image from path\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    # normalize image pixels\n",
    "    img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(img_dir, img_size=(256,144)):\n",
    "\n",
    "    \"\"\"\n",
    "        Function to coordinate the load of all the images that are going to be used.\n",
    "        \n",
    "        :param: img_dir - path to the directory containing the images\n",
    "        :param: img_size - image size\n",
    "        \n",
    "        :return: images_set - numpy array with all images at the set\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print (\"|    Loading images from: \", img_dir)\n",
    "    \n",
    "    # loop to read all the images of the directory\n",
    "    images = [get_image(img,img_size) for img in glob.glob(img_dir+'/*')]\n",
    "    \n",
    "    # normalize images\n",
    "    images = mean_normalize(images)\n",
    "    \n",
    "    #resemble images as RGB\n",
    "    images = [img[:, :, (2, 1, 0)] for img in images]\n",
    "    \n",
    "    #Transpose the image that channels num. as first dimension\n",
    "    images = [np.transpose(img,(2,0,1)) for img in images]\n",
    "    images = [torch.from_numpy(img) for img in images]\n",
    "    \n",
    "    #stack per 2 images\n",
    "    images = [np.concatenate((images[k],images[k+1]),axis = 0) for k in range(len(images)-1)]\n",
    "        \n",
    "    print(\"|    Images count : \",len(images))\n",
    "\n",
    "    # reshape the array of all images\n",
    "    images_set = np.stack(images,axis=0)\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "    return images_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions are used to the Kitti Dataset poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRotationMatrix(R):\n",
    "    \"\"\" \n",
    "        Checks if a matrix is a valid rotation matrix referred from \n",
    "        https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "        \n",
    "        :param: R - rotation matrix\n",
    "        \n",
    "        :return: True or False\n",
    "        \n",
    "    \"\"\"\n",
    "    # calc the transpose\n",
    "    Rt = np.transpose(R)\n",
    "\n",
    "    # check identity\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    \n",
    "    return n < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\" \n",
    "        Calculates rotation matrix to euler angles\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles\n",
    "        \n",
    "        :param: R - rotation matrix\n",
    "        \n",
    "        :return: rotation matrix for Euler angles\n",
    "    \"\"\"\n",
    "    assert(isRotationMatrix(R))\n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrices(all_poses):\n",
    "    \"\"\"\n",
    "        Function to extract matrices from poses\n",
    "        \n",
    "        :param: all_poses - list with all poses from the sequence\n",
    "        \n",
    "        :return: all_matrices - list with all matrices obtained from the poses\n",
    "    \"\"\"\n",
    "    all_matrices = []\n",
    "    for i in range(len(all_poses)):\n",
    "        #print(\"I: \",i)\n",
    "        j = all_poses[i]\n",
    "        #print(\"J:   \",j)\n",
    "        p = np.array([j[3], j[7], j[11]])\n",
    "        #print(\"P:   \", p)\n",
    "        R = np.array([[j[0],j[1],j[2]],\n",
    "                      [j[4],j[5],j[6]],\n",
    "                      [j[8],j[9],j[10]]\n",
    "                     ])\n",
    "        #print(\"R:   \", R)\n",
    "        angles = rotationMatrixToEulerAngles(R)\n",
    "        #print(\"Angles: \",angles)\n",
    "        matrix = np.concatenate((p,angles))\n",
    "        #print(\"MATRIX: \", matrix)\n",
    "        all_matrices.append(matrix)\n",
    "    return all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kitti_images(pose_file):\n",
    "    poses = []\n",
    "    poses_set = []\n",
    "\n",
    "    with open(pose_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose = np.fromstring(line, dtype=float, sep=' ')\n",
    "            poses.append(pose)\n",
    "    \n",
    "    poses = getMatrices(poses)\n",
    "    pose1 = poses[0]\n",
    "    for i in range(len(poses)-1):\n",
    "        pose2 = poses[i+1]\n",
    "        finalpose = np.zeros(pose1)\n",
    "        poses_set.append(finalpose)\n",
    "\n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions are used for the poses obtained from the AirSim simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pi_to_poses(pose):\n",
    "    '''Add Pi to every pose angle.'''\n",
    "    pose += np.pi\n",
    "    \n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_to_euler_angles(quat_matrix):\n",
    "    # create a scipy object from the quaternion angles\n",
    "    rot_mat = R.from_quat(quat_matrix)\n",
    "    # convert the quaternion to euler (in degrees)\n",
    "    euler_mat = rot_mat.as_euler('yxz', degrees=False)\n",
    "    # convert from (-pi,pi) to (0,2pi)\n",
    "    euler_convert = add_pi_to_poses(euler_mat)\n",
    "    \n",
    "    return euler_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_airsim_pose(pose_file):\n",
    "    \n",
    "    poses = []\n",
    "    poses_set = []\n",
    "    \n",
    "    df_poses = pd.read_csv(pose_file)\n",
    "    for index, row in df_poses.iterrows():\n",
    "        # get the (x,y,z) positions of the camera\n",
    "        position = np.array([row['POS_X'],row['POS_Y'],row['POS_Z']])\n",
    "        # get the quaternions angles of the camera\n",
    "        quat_matrix = np.array([row['Q_X'],row['Q_Y'], row['Q_Z'],row['Q_W']])\n",
    "        # call the func that convert the quaternions to euler angles\n",
    "        euler_matrix = quat_to_euler_angles(quat_matrix)\n",
    "        # concatenate both position(x,y,z) and euler angles\n",
    "        poses.append(np.concatenate((position,euler_matrix)))\n",
    "    \n",
    "    # make the first pose as start position\n",
    "    pose1 = poses[0]\n",
    "    for i in range(len(poses)):\n",
    "        pose2 = poses[i]\n",
    "        pose_diff = np.subtract(pose2, pose1)\n",
    "        pose_diff[4:] = np.arctan2(np.sin(pose_diff[4:]), np.cos(pose_diff[4:]))\n",
    "        \n",
    "        poses[i] = pose_diff\n",
    "    \n",
    "    # get the desloc between two poses\n",
    "    for i in range(len(poses)-1):\n",
    "        pose1 = poses[i]\n",
    "        pose2 = poses[i+1]\n",
    "\n",
    "        pose_diff = np.subtract(pose2, pose1)\n",
    "        pose_diff[4:] = np.arctan2(np.sin(pose_diff[4:]), np.cos(pose_diff[4:]))\n",
    "\n",
    "        poses_set.append(pose_diff)\n",
    "        \n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poses(pose_file, pose_format='airsim'):\n",
    "    \"\"\"\n",
    "        Function to load the image poses.\n",
    "        \n",
    "        :param: pose_file - path to the pose file\n",
    "        :param: pose_format - where the pose were obtained from (AirSim, VREP, Kitti, etc...)\n",
    "        \n",
    "        :return: pose_set - set of the poses for the sequence\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print (\"|    Pose from: \",pose_file)\n",
    "    \n",
    "    if pose_format.lower() == 'kitti':\n",
    "        poses_set = load_kitti_images(pose_file)\n",
    "    elif pose_format.lower() == 'airsim':\n",
    "        poses_set = load_airsim_pose(pose_file)\n",
    "        \n",
    "    print(\"|        Poses count: \",len(poses_set))\n",
    "    print(\"----------------------------------------------------------------------\")    \n",
    "    return poses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "Function that acquire all data that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VODataLoader(datapath,img_size=(256,144), test=False):\n",
    "    if test:\n",
    "        sequences = ['3']\n",
    "    else:\n",
    "        sequences = ['1','2','4','5','6']\n",
    "        \n",
    "    images_set = []\n",
    "    odometry_set = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        dir_path = os.path.join(datapath,'seq'+sequence)\n",
    "        image_path = os.path.join(dir_path,'images')\n",
    "        pose_path = os.path.join(dir_path,'poses.csv')\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        print(\"|Load from: \", dir_path)\n",
    "        images_set.append(torch.FloatTensor(load_images(image_path,img_size)))\n",
    "        odometry_set.append(torch.FloatTensor(load_poses(pose_path, 'AirSim')))\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"|   Total Images: \", len(images_set))\n",
    "    print(\"|   Total Odometry: \", len(odometry_set))\n",
    "    print(\"---------------------------------------------------\")    \n",
    "    return images_set, odometry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq1\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq1/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.3115534  0.29183614 0.27286035]\n",
      "|    Done\n",
      "|    Images count :  326\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq1/poses.csv\n",
      "|        Poses count:  326\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq2\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq2/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.2615339  0.24993458 0.2350733 ]\n",
      "|    Done\n",
      "|    Images count :  283\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq2/poses.csv\n",
      "|        Poses count:  283\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq4\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq4/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.34079915 0.32100374 0.28959265]\n",
      "|    Done\n",
      "|    Images count :  368\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq4/poses.csv\n",
      "|        Poses count:  368\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq5\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq5/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.13811225 0.20660812 0.3470334 ]\n",
      "|    Done\n",
      "|    Images count :  121\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq5/poses.csv\n",
      "|        Poses count:  121\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "|Load from:  dataset/seq6\n",
      "----------------------------------------------------------------------\n",
      "|    Loading images from:  dataset/seq6/images\n",
      "|    Mean-normalizing ...\n",
      "|    Mean: [0.59857106 0.6014284  0.56355304]\n",
      "|    Done\n",
      "|    Images count :  208\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "|    Pose from:  dataset/seq6/poses.csv\n",
      "|        Poses count:  208\n",
      "----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "|   Total Images:  5\n",
      "|   Total Odometry:  5\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X,y = VODataLoader(datapath='dataset', test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting lists containing tensors to tensors as per the batchsize (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [item for x in X for item in x]\n",
    "Y_train = [item for a in y for item in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info about the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Details of X :\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "1306\n",
      "torch.Size([6, 144, 256])\n",
      "---------------------------------\n",
      "Details of y :\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "1306\n",
      "torch.Size([6])\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------\")\n",
    "print(\"Details of X :\")\n",
    "print(type(X_train)) \n",
    "print(type(X_train[0]))\n",
    "print(len(X_train)) \n",
    "print(X_train[0].size())\n",
    "print(\"---------------------------------\")\n",
    "print(\"Details of y :\")\n",
    "print(type(Y_train))\n",
    "print(type(Y_train[0]))\n",
    "print(len(Y_train))\n",
    "print(Y_train[0].size())\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = torch.stack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stack = torch.stack(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = X_stack.view(-1,1,6,144,256)\n",
    "y_batch = y_stack.view(-1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([1306, 1, 6, 144, 256])\n",
      "Details of y :\n",
      "torch.Size([1306, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Details of X :\")\n",
    "print(X_batch.size())\n",
    "print(\"Details of y :\")\n",
    "print(y_batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .2\n",
    "dataset_size = len(X_batch)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch_train = X_batch[split:]\n",
    "y_batch_train = y_batch[split:]\n",
    "X_batch_validation = X_batch[:split]\n",
    "y_batch_validation = y_batch[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining DeepVO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepVONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepVONet, self).__init__()\n",
    "        # CNN\n",
    "        # convolutional layer 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False), #6 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2), \n",
    "        )\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 3_1        \n",
    "        self.conv3_1 = nn.Sequential(\n",
    "            nn.Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 4_1\n",
    "        self.conv4_1 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 5\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 5_1\n",
    "        self.conv5_1 = nn.Sequential(\n",
    "            nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # convolutional layer 6\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d (512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        # RNN\n",
    "        self.lstm1 = nn.LSTMCell(2*6*1024, 100)\n",
    "        self.lstm2 = nn.LSTMCell(100, 100)\n",
    "        self.fc = nn.Linear(in_features=100, out_features=6)\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def reset_hidden_states(self, size=1, zero=True):\n",
    "        if zero == True:\n",
    "            self.hx1 = Variable(torch.zeros(size, 100))\n",
    "            self.cx1 = Variable(torch.zeros(size, 100))\n",
    "            self.hx2 = Variable(torch.zeros(size, 100))\n",
    "            self.cx2 = Variable(torch.zeros(size, 100))\n",
    "        else:\n",
    "            self.hx1 = Variable(self.hx1.data)\n",
    "            self.cx1 = Variable(self.cx1.data)\n",
    "            self.hx2 = Variable(self.hx2.data)\n",
    "            self.cx2 = Variable(self.cx2.data)\n",
    "\n",
    "        if next(self.parameters()).is_cuda == True:\n",
    "            self.hx1 = self.hx1.cuda()\n",
    "            self.cx1 = self.cx1.cuda()\n",
    "            self.hx2 = self.hx2.cuda()\n",
    "            self.cx2 = self.cx2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = x.view(x.size(0), 2 * 6 * 1024)\n",
    "        \n",
    "        self.hx1, self.cx1 = self.lstm1(x, (self.hx1, self.cx1))\n",
    "        x = self.hx1\n",
    "        self.hx2, self.cx2 = self.lstm2(x, (self.hx2, self.cx2))\n",
    "        x = self.hx2\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVONet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv3_1): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv4_1): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv5_1): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (lstm1): LSTMCell(12288, 100)\n",
      "  (lstm2): LSTMCell(100, 100)\n",
      "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = DeepVONet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss and optimizer to be used \n",
    "criterion = torch.nn.MSELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, weight_decay=0.5)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the pretrained weight of FlowNet ( CNN part )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained = torch.load('flownets_EPE1.951.pth.tar',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dict['conv1.weight'] = pre_trained['state_dict']['conv1.0.weight']\n",
    "update_dict['conv2.weight'] = pre_trained['state_dict']['conv2.0.weight']\n",
    "update_dict['conv3.weight'] = pre_trained['state_dict']['conv3.0.weight']\n",
    "update_dict['conv4.weight'] = pre_trained['state_dict']['conv4.0.weight']\n",
    "update_dict['conv4_1.weight'] = pre_trained['state_dict']['conv4_1.0.weight']\n",
    "update_dict['conv5.weight'] = pre_trained['state_dict']['conv5.0.weight']\n",
    "update_dict['conv5_1.weight'] = pre_trained['state_dict']['conv5_1.0.weight']\n",
    "update_dict['conv6.weight'] = pre_trained['state_dict']['conv6.0.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(update_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, train_num, X, y, epoch_num=25):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        for i in range(train_num):\n",
    "            print(\"    Train num :\", i+1)\n",
    "            inputs = X[i]\n",
    "            print(\"        Input Size: {}\".format(inputs.size()))\n",
    "            labels = y[i]\n",
    "            print(\"        Labels: \",labels)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            print(\"        Outputs: \",outputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('    Epoch : %d Loss: %.3f' %(epoch+1, running_loss/train_num))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    print (\"Time taken in Training {0}\".format((time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_v2(model, X_train, y_train, X_validate, y_validate, num_epochs):\n",
    "    # start model train mode\n",
    "    train_ep_loss = []\n",
    "    valid_ep_loss = []\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "        st_t = time.time()\n",
    "        print('='*50)\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        \n",
    "        loss_mean = 0\n",
    "        t_loss_list = []\n",
    "        \n",
    "        for i in range(X_train.size(0)):\n",
    "            # get the images inputs\n",
    "            inputs = X_train[i]\n",
    "            # get the original poses\n",
    "            labels = y_train[i]\n",
    "            # zero optimizer\n",
    "            optimizer.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "            \n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            # get mse loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            ls = loss.item()\n",
    "            loss.backward()\n",
    "            # set next optimizer step\n",
    "            optimizer.step()\n",
    "            # append loss\n",
    "            t_loss_list.append(float(ls))\n",
    "            # update loss\n",
    "            loss_mean += float(ls)\n",
    "        \n",
    "        print('Train take {:.1f} sec'.format(time.time()-st_t))\n",
    "        loss_mean /= (X_train.size(0))\n",
    "        train_ep_loss.append(loss_mean)\n",
    "        \n",
    "        # Validation\n",
    "        st_t = time.time()\n",
    "        model.eval()\n",
    "        loss_mean_valid = 0\n",
    "        v_loss_list = []\n",
    "        \n",
    "        for i in range(X_validate.size(0)):\n",
    "            # get the images inputs\n",
    "            inputs = X_validate[i]\n",
    "            # get the original poses\n",
    "            labels = y_validate[i]\n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            # get mse loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            ls = loss.item()\n",
    "            # update loss values\n",
    "            v_loss_list.append(float(ls))\n",
    "            loss_mean_valid += float(ls)\n",
    "        \n",
    "        print('Valid take {:.1f} sec'.format(time.time()-st_t))\n",
    "        loss_mean_valid /= X_validate.size(0)\n",
    "        valid_ep_loss.append(loss_mean_valid)\n",
    "        \n",
    "        print('Epoch {}\\ntrain loss mean: {}, std: {:.2f}\\nvalid loss mean: {}, std: {:.2f}\\n'.format(ep+1, loss_mean, np.std(t_loss_list), loss_mean_valid, np.std(v_loss_list)))\n",
    "        \n",
    "    return train_ep_loss, valid_ep_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Train take 535.4 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 1\n",
      "train loss mean: 0.6120069708051606, std: 1.42\n",
      "valid loss mean: 0.5037028984063201, std: 0.97\n",
      "\n",
      "==================================================\n",
      "Train take 480.4 sec\n",
      "Valid take 20.6 sec\n",
      "Epoch 2\n",
      "train loss mean: 0.6020250072106763, std: 1.38\n",
      "valid loss mean: 0.5382052420627796, std: 0.92\n",
      "\n",
      "==================================================\n",
      "Train take 425.7 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 3\n",
      "train loss mean: 0.5784835852024204, std: 1.32\n",
      "valid loss mean: 0.6847623754506824, std: 0.88\n",
      "\n",
      "==================================================\n",
      "Train take 421.5 sec\n",
      "Valid take 20.2 sec\n",
      "Epoch 4\n",
      "train loss mean: 0.5806255663086718, std: 1.33\n",
      "valid loss mean: 0.71781809787869, std: 0.87\n",
      "\n",
      "==================================================\n",
      "Train take 429.4 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 5\n",
      "train loss mean: 0.572138224961626, std: 1.28\n",
      "valid loss mean: 0.5061345360530861, std: 0.91\n",
      "\n",
      "==================================================\n",
      "Train take 438.7 sec\n",
      "Valid take 21.1 sec\n",
      "Epoch 6\n",
      "train loss mean: 0.5497456811983963, std: 1.25\n",
      "valid loss mean: 0.6617304085828792, std: 0.87\n",
      "\n",
      "==================================================\n",
      "Train take 430.2 sec\n",
      "Valid take 19.8 sec\n",
      "Epoch 7\n",
      "train loss mean: 0.5435410701305468, std: 1.22\n",
      "valid loss mean: 0.6577828827945665, std: 0.90\n",
      "\n",
      "==================================================\n",
      "Train take 444.8 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 8\n",
      "train loss mean: 0.541159311330679, std: 1.21\n",
      "valid loss mean: 0.5732616781760221, std: 0.94\n",
      "\n",
      "==================================================\n",
      "Train take 421.2 sec\n",
      "Valid take 19.5 sec\n",
      "Epoch 9\n",
      "train loss mean: 0.5392896233895197, std: 1.21\n",
      "valid loss mean: 0.6206923697243705, std: 1.12\n",
      "\n",
      "==================================================\n",
      "Train take 434.4 sec\n",
      "Valid take 20.4 sec\n",
      "Epoch 10\n",
      "train loss mean: 0.5329891622662778, std: 1.18\n",
      "valid loss mean: 0.5545074785934668, std: 0.94\n",
      "\n",
      "==================================================\n",
      "Train take 420.6 sec\n",
      "Valid take 21.0 sec\n",
      "Epoch 11\n",
      "train loss mean: 0.5408627741755673, std: 1.18\n",
      "valid loss mean: 0.5941278610835244, std: 1.09\n",
      "\n",
      "==================================================\n",
      "Train take 428.3 sec\n",
      "Valid take 20.8 sec\n",
      "Epoch 12\n",
      "train loss mean: 0.5315903054128076, std: 1.17\n",
      "valid loss mean: 0.6212379777539964, std: 1.12\n",
      "\n",
      "==================================================\n",
      "Train take 437.4 sec\n",
      "Valid take 20.1 sec\n",
      "Epoch 13\n",
      "train loss mean: 0.5352308432115955, std: 1.17\n",
      "valid loss mean: 0.5767351524882961, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 480.7 sec\n",
      "Valid take 25.3 sec\n",
      "Epoch 14\n",
      "train loss mean: 0.5388806583349254, std: 1.17\n",
      "valid loss mean: 0.6763344940504638, std: 1.20\n",
      "\n",
      "==================================================\n",
      "Train take 437.4 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 15\n",
      "train loss mean: 0.5215715155082322, std: 1.15\n",
      "valid loss mean: 0.5794830571439017, std: 1.05\n",
      "\n",
      "==================================================\n",
      "Train take 426.5 sec\n",
      "Valid take 20.0 sec\n",
      "Epoch 16\n",
      "train loss mean: 0.5195946306293233, std: 1.14\n",
      "valid loss mean: 0.620343649215725, std: 1.04\n",
      "\n",
      "==================================================\n",
      "Train take 423.8 sec\n",
      "Valid take 19.6 sec\n",
      "Epoch 17\n",
      "train loss mean: 0.5229175951420418, std: 1.15\n",
      "valid loss mean: 0.6242135393440409, std: 1.04\n",
      "\n",
      "==================================================\n",
      "Train take 423.0 sec\n",
      "Valid take 19.7 sec\n",
      "Epoch 18\n",
      "train loss mean: 0.5319211244124152, std: 1.14\n",
      "valid loss mean: 0.6675578626943217, std: 1.18\n",
      "\n",
      "==================================================\n",
      "Train take 418.5 sec\n",
      "Valid take 19.8 sec\n",
      "Epoch 19\n",
      "train loss mean: 0.5102410368294383, std: 1.12\n",
      "valid loss mean: 0.6635665043410613, std: 1.15\n",
      "\n",
      "==================================================\n",
      "Train take 450.9 sec\n",
      "Valid take 22.8 sec\n",
      "Epoch 20\n",
      "train loss mean: 0.5110517673146227, std: 1.11\n",
      "valid loss mean: 0.6447619553744594, std: 1.02\n",
      "\n",
      "==================================================\n",
      "Train take 447.8 sec\n",
      "Valid take 21.1 sec\n",
      "Epoch 21\n",
      "train loss mean: 0.5102599773141856, std: 1.11\n",
      "valid loss mean: 0.6543519765823737, std: 0.97\n",
      "\n",
      "==================================================\n",
      "Train take 436.8 sec\n",
      "Valid take 23.9 sec\n",
      "Epoch 22\n",
      "train loss mean: 0.5070249447255935, std: 1.10\n",
      "valid loss mean: 0.5996863867523741, std: 1.12\n",
      "\n",
      "==================================================\n",
      "Train take 427.2 sec\n",
      "Valid take 22.6 sec\n",
      "Epoch 23\n",
      "train loss mean: 0.501903121195704, std: 1.09\n",
      "valid loss mean: 0.5842696247984551, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 439.2 sec\n",
      "Valid take 20.3 sec\n",
      "Epoch 24\n",
      "train loss mean: 0.4897542449781887, std: 1.07\n",
      "valid loss mean: 0.6455123899000522, std: 1.06\n",
      "\n",
      "==================================================\n",
      "Train take 454.2 sec\n",
      "Valid take 25.9 sec\n",
      "Epoch 25\n",
      "train loss mean: 0.4988019143174955, std: 1.08\n",
      "valid loss mean: 0.638240823171626, std: 1.17\n",
      "\n",
      "==================================================\n",
      "Train take 434.7 sec\n",
      "Valid take 20.2 sec\n",
      "Epoch 26\n",
      "train loss mean: 0.49682085234371043, std: 1.08\n",
      "valid loss mean: 0.6330184826950304, std: 1.15\n",
      "\n",
      "==================================================\n",
      "Train take 426.5 sec\n",
      "Valid take 19.7 sec\n",
      "Epoch 27\n",
      "train loss mean: 0.48953734200412147, std: 1.07\n",
      "valid loss mean: 0.6319951262473132, std: 1.14\n",
      "\n",
      "==================================================\n",
      "Train take 410.6 sec\n",
      "Valid take 19.2 sec\n",
      "Epoch 28\n",
      "train loss mean: 0.48683460445149407, std: 1.07\n",
      "valid loss mean: 0.6256845816318659, std: 1.13\n",
      "\n",
      "==================================================\n",
      "Train take 428.8 sec\n",
      "Valid take 19.4 sec\n",
      "Epoch 29\n",
      "train loss mean: 0.48311730867976427, std: 1.06\n",
      "valid loss mean: 0.6248653099294109, std: 1.15\n",
      "\n",
      "==================================================\n",
      "Train take 426.3 sec\n",
      "Valid take 20.8 sec\n",
      "Epoch 30\n",
      "train loss mean: 0.4776585401453186, std: 1.05\n",
      "valid loss mean: 0.5973296739639165, std: 1.10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = training_model_v2(model, X_batch_train, y_batch_train, X_batch_validation, y_batch_validation, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(30)),train_loss, label='Training Loss')\n",
    "plt.plot(list(range(30)),valid_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('images_model/train_batch_dropout_30.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_loss = [train_loss, valid_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model (model, test_num, X):\n",
    "    start_time = time.time()\n",
    "    Y_output = []\n",
    "    count = 0\n",
    "    totcount = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(test_num):\n",
    "            # get the inputs\n",
    "            inputs = X[i]\n",
    "            outputs = model(inputs)\n",
    "            Y_output.append(outputs)\n",
    "\n",
    "    print (\"Time taken in Testing {0}\".format((time.time() - start_time)))\n",
    "    return torch.stack(Y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X,y = VODataLoader(datapath='dataset', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0] = X[0].view(-1,1,6,144,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = testing_model(model,X[0].size(0),X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[0].view(-1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to get accuracy\n",
    "def get_accuracy(outputs, labels, batch_size):\n",
    "    diff =0\n",
    "    for i in range(batch_size):\n",
    "        out = (outputs[i].detach().numpy())[0]\n",
    "        lab = (labels[i].detach().numpy())[0]\n",
    "\n",
    "        diff+=get_mse_diff(out,lab)\n",
    "        \n",
    "    print(\"Accuracy : \",(1 -diff/(batch_size))*100,\"%\")\n",
    "    \n",
    "def get_mse_diff(x,y):\n",
    "    diff= 0\n",
    "    for i in range(6):\n",
    "        diff += (x[i]-y[i]) ** 2\n",
    "    return diff/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(y_out, y, 497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/deepvo-dropout.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (outputs[j].detach().numpy())[0]\n",
    "x_ori = []\n",
    "y_ori = []\n",
    "for i in range(1,len(y)):\n",
    "    out = y[i]\n",
    "    out = (out.detach().numpy())[0]\n",
    "    x_ori.append(out[0])\n",
    "    y_ori.append(out[0])    \n",
    "#     x_ori.append(x_ori[i-1] + out[0])\n",
    "#     y_ori.append(y_ori[i-1] + out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o = []\n",
    "y_o = []\n",
    "for i in range(len(y_out)):    \n",
    "    out = y_out[i]\n",
    "    out = (out.detach().numpy())[0]\n",
    "    x_o.append(out[0])\n",
    "    y_o.append(out[1])\n",
    "#     x_o.append(x_o[i-1] + out[0])\n",
    "#     y_o.append(y_o[i-1] + out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.cumsum(x_ori),np.cumsum(y_ori))\n",
    "plt.scatter(x_ori[0],y_ori[0],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.cumsum(x_o),np.cumsum(y_o))\n",
    "plt.scatter(x_o[0],y_o[0],marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
