\section{Introduction}
        Visual Odometry (VO) is one of the most essencial techniques for pose estimation and robot localization now a days. The classical approach for VO systems is shown in Figure \ref{fig:1}, which typically consists of camera calibration, feature detection, feature matching, outlier rejection, scale estimation and local optimisation, has been developed and broadly recognised as a golden rule to follow. Altough the state-of-the-art algorithms based on this pipeline have shown excellent performance, they are usually hard-coded, where each module of the pipeline is adapted to the hardware in order to achieve an amazing performance. 
        
        In parallel, in the last years, Deep Learning (DL) \cite{lecun2015deep}, has been dominating many computer vision tasks with spectacular results. Unfortunately, for the VO problem this has not fully arrived yet, walking in short steps. In this work, we are going to present some new approachs that uses DL for the VO problem. Most of them consist at a Deep Recurrent Convolutional Neural Networks to solve the problem. One of the first articles that addresses the solution is shown at \cite{wang2017deepvo} and a pipeline is shown at Figure \ref{fig:2}.
        
        Another approach that has been gaining attention, is the use of a visual saliency map in order to increase the performance of the state-of-the-art algorithms. The concept is quite simple, as we are going to futher explain, humans perform the task of mapping very differently from how it has been usually done at the classical approaches at robotics \cite{liang2019salientdso}.
        
        For this work, we aim to focus our efforts at the Deep Learning approach. This paper is organized as follows: Section \ref{sec:trabalho-proposto}  presents how the paper was organized in order to solve the problem. Section \ref{sec:metodos} describes the tests done and the methodologies covered. The results and analysis are presented in Section \ref{sec:res-dic} and finally, the conclusions at Section \ref{sec:conclusao}.